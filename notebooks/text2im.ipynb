{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengxiaozhibo/AlgoNotes/blob/main/notebooks/text2im.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q0hHHs9zd0E"
      },
      "outputs": [],
      "source": [
        "# Run this line in Colab to install the package if it is\n",
        "# not already installed.\n",
        "!pip install git+https://github.com/openai/glide-text2im"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def apply_category_scatter(seq, category_seq):\n",
        "    \"\"\"\n",
        "    类目打散函数\n",
        "\n",
        "    Args:\n",
        "        seq: 商品序列 [batch_size, seq_len]\n",
        "        category_seq: 类目序列 [batch_size, seq_len]\n",
        "\n",
        "    Returns:\n",
        "        scattered_seq: 打散后的商品序列 [batch_size, seq_len]\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(seq)[0]\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "\n",
        "    def scatter_single_batch(args):\n",
        "        batch_seq, batch_category = args\n",
        "        valid_mask = tf.greater_equal(batch_seq, 0)\n",
        "        valid_indices = tf.where(valid_mask)\n",
        "        valid_count = tf.shape(valid_indices)[0]\n",
        "\n",
        "        # 如果没有有效元素，直接返回原序列\n",
        "        def no_valid_items():\n",
        "            return batch_seq\n",
        "\n",
        "        def has_valid_items():\n",
        "            # 提取有效元素\n",
        "            valid_items = tf.gather_nd(batch_seq, valid_indices)\n",
        "            valid_categories = tf.gather_nd(batch_category, valid_indices)\n",
        "\n",
        "            # 按类目分组并保持原有顺序\n",
        "            unique_categories, _ = tf.unique(valid_categories)\n",
        "            category_groups = []\n",
        "\n",
        "            # 使用 tf.while_loop 来处理动态循环\n",
        "            def group_by_category(i, groups):\n",
        "                if i >= tf.shape(unique_categories)[0]:\n",
        "                    return i, groups\n",
        "\n",
        "                cat = unique_categories[i]\n",
        "                cat_mask = tf.equal(valid_categories, cat)\n",
        "                cat_items = tf.boolean_mask(valid_items, cat_mask)\n",
        "                groups.append(cat_items)\n",
        "                return i + 1, groups\n",
        "\n",
        "            # 简化版本：使用 Python 循环（在 graph 模式下可能需要调整）\n",
        "            for i in tf.range(tf.shape(unique_categories)[0]):\n",
        "                cat = unique_categories[i]\n",
        "                cat_mask = tf.equal(valid_categories, cat)\n",
        "                cat_items = tf.boolean_mask(valid_items, cat_mask)\n",
        "                category_groups.append(cat_items)\n",
        "\n",
        "            # 轮询式打散：从每个类目组轮流取一个元素\n",
        "            scattered_items = []\n",
        "            group_indices = [0] * len(category_groups)\n",
        "\n",
        "            # 计算总元素数\n",
        "            total_items = tf.shape(valid_items)[0]\n",
        "\n",
        "            # 使用 tf.while_loop 实现轮询\n",
        "            def collect_items(step, items, indices):\n",
        "                if step >= total_items:\n",
        "                    return step, items, indices\n",
        "\n",
        "                # 遍历所有类目组\n",
        "                def process_groups(group_idx, step, items, indices):\n",
        "                    if group_idx >= len(category_groups):\n",
        "                        return group_idx, step, items, indices\n",
        "\n",
        "                    group_size = tf.shape(category_groups[group_idx])[0]\n",
        "                    current_idx = indices[group_idx]\n",
        "\n",
        "                    def add_item():\n",
        "                        item = category_groups[group_idx][current_idx]\n",
        "                        new_items = items + [item]\n",
        "                        new_indices = indices[:group_idx] + [current_idx + 1] + indices[group_idx + 1:]\n",
        "                        return new_items, new_indices\n",
        "\n",
        "                    def skip_group():\n",
        "                        return items, indices\n",
        "\n",
        "                    new_items, new_indices = tf.cond(\n",
        "                        current_idx < group_size,\n",
        "                        add_item,\n",
        "                        skip_group\n",
        "                    )\n",
        "\n",
        "                    return process_groups(group_idx + 1, step + 1, new_items, new_indices)\n",
        "\n",
        "                return process_groups(0, step, items, indices)\n",
        "\n",
        "            # 执行轮询收集\n",
        "            _, scattered_items, _ = collect_items(0, [], group_indices)\n",
        "\n",
        "            # 重新填充到原始位置\n",
        "            scattered_seq = tf.zeros_like(batch_seq)\n",
        "            scattered_seq = tf.tensor_scatter_nd_update(scattered_seq, valid_indices, scattered_items)\n",
        "\n",
        "            return scattered_seq\n",
        "\n",
        "        return tf.cond(tf.equal(valid_count, 0), no_valid_items, has_valid_items)\n",
        "\n",
        "    scattered_seq = tf.map_fn(scatter_single_batch, (seq, category_seq), dtype=seq.dtype)\n",
        "    return scattered_seq\n",
        "\n",
        "\n",
        "# ===== 调试示例代码 =====\n",
        "\n",
        "def test_apply_category_scatter():\n",
        "    \"\"\"测试类目打散函数\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"测试类目打散函数\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 测试用例1：简单的类目打散\n",
        "    print(\"\\n【测试用例1：简单的类目打散】\")\n",
        "    seq1 = tf.constant([\n",
        "        [1, 2, 3, 4, 5, 6],  # batch 0\n",
        "        [7, 8, 9, 10, 11, 12]  # batch 1\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    category_seq1 = tf.constant([\n",
        "        [1, 1, 2, 2, 3, 3],  # batch 0: 类目1,1,2,2,3,3\n",
        "        [1, 2, 1, 2, 1, 2]   # batch 1: 类目1,2,1,2,1,2\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    print(\"原始序列:\")\n",
        "    print(seq1.numpy())\n",
        "    print(\"类目序列:\")\n",
        "    print(category_seq1.numpy())\n",
        "\n",
        "    result1 = apply_category_scatter(seq1, category_seq1)\n",
        "    print(\"打散后序列:\")\n",
        "    print(result1.numpy())\n",
        "    print(\"打散后类目:\")\n",
        "    # 需要重新获取打散后的类目\n",
        "    print(\"(需要根据打散后的索引重新映射类目)\")\n",
        "\n",
        "    # 测试用例2：包含无效元素（-1或0）\n",
        "    print(\"\\n【测试用例2：包含无效元素】\")\n",
        "    seq2 = tf.constant([\n",
        "        [1, 2, 3, -1, 5, -1],  # batch 0: 包含-1\n",
        "        [7, -1, 9, 10, -1, 12]  # batch 1: 包含-1\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    category_seq2 = tf.constant([\n",
        "        [1, 1, 2, -1, 3, -1],\n",
        "        [1, -1, 2, 2, -1, 3]\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    print(\"原始序列:\")\n",
        "    print(seq2.numpy())\n",
        "    print(\"类目序列:\")\n",
        "    print(category_seq2.numpy())\n",
        "\n",
        "    result2 = apply_category_scatter(seq2, category_seq2)\n",
        "    print(\"打散后序列:\")\n",
        "    print(result2.numpy())\n",
        "\n",
        "    # 测试用例3：单个类目\n",
        "    print(\"\\n【测试用例3：单个类目】\")\n",
        "    seq3 = tf.constant([\n",
        "        [1, 2, 3, 4, 5],\n",
        "        [6, 7, 8, 9, 10]\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    category_seq3 = tf.constant([\n",
        "        [1, 1, 1, 1, 1],  # 全部是类目1\n",
        "        [2, 2, 2, 2, 2]   # 全部是类目2\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    print(\"原始序列:\")\n",
        "    print(seq3.numpy())\n",
        "    print(\"类目序列:\")\n",
        "    print(category_seq3.numpy())\n",
        "\n",
        "    result3 = apply_category_scatter(seq3, category_seq3)\n",
        "    print(\"打散后序列:\")\n",
        "    print(result3.numpy())\n",
        "\n",
        "    # 测试用例4：模拟你的实际场景\n",
        "    print(\"\\n【测试用例4：模拟实际场景】\")\n",
        "    # 假设有3个类目，每个类目有多个商品\n",
        "    seq4 = tf.constant([\n",
        "        [101, 102, 201, 202, 301, 302, 103, 203],  # batch 0\n",
        "        [401, 402, 501, 502, 601, 602, 403, 503]   # batch 1\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    category_seq4 = tf.constant([\n",
        "        [1, 1, 2, 2, 3, 3, 1, 2],  # 类目分布\n",
        "        [4, 4, 5, 5, 6, 6, 4, 5]\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    print(\"原始序列:\")\n",
        "    print(seq4.numpy())\n",
        "    print(\"类目序列:\")\n",
        "    print(category_seq4.numpy())\n",
        "\n",
        "    result4 = apply_category_scatter(seq4, category_seq4)\n",
        "    print(\"打散后序列:\")\n",
        "    print(result4.numpy())\n",
        "    print(\"预期结果：应该按类目轮询排列，如 101,201,301,102,202,302,103,203\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"测试完成\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# 简化版本的类目打散函数（更易于调试）\n",
        "def apply_category_scatter_simple(seq, category_seq):\n",
        "    \"\"\"\n",
        "    简化版类目打散函数，使用纯Python逻辑（仅用于调试理解）\n",
        "\n",
        "    注意：这个版本不能在TensorFlow graph中使用，仅用于理解算法逻辑\n",
        "    \"\"\"\n",
        "    batch_size = seq.shape[0]\n",
        "    seq_len = seq.shape[1]\n",
        "\n",
        "    scattered_seq = np.zeros_like(seq.numpy())\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        batch_seq = seq[b].numpy()\n",
        "        batch_category = category_seq[b].numpy()\n",
        "\n",
        "        # 找到有效元素\n",
        "        valid_mask = batch_seq >= 0\n",
        "        valid_indices = np.where(valid_mask)[0]\n",
        "        valid_items = batch_seq[valid_indices]\n",
        "        valid_categories = batch_category[valid_indices]\n",
        "\n",
        "        if len(valid_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # 按类目分组\n",
        "        category_dict = {}\n",
        "        for item, cat in zip(valid_items, valid_categories):\n",
        "            if cat not in category_dict:\n",
        "                category_dict[cat] = []\n",
        "            category_dict[cat].append(item)\n",
        "\n",
        "        # 轮询式打散\n",
        "        scattered_items = []\n",
        "        group_indices = {cat: 0 for cat in category_dict.keys()}\n",
        "\n",
        "        while len(scattered_items) < len(valid_items):\n",
        "            for cat in sorted(category_dict.keys()):\n",
        "                if group_indices[cat] < len(category_dict[cat]):\n",
        "                    scattered_items.append(category_dict[cat][group_indices[cat]])\n",
        "                    group_indices[cat] += 1\n",
        "                    if len(scattered_items) >= len(valid_items):\n",
        "                        break\n",
        "\n",
        "        # 填充回原位置\n",
        "        scattered_seq[b][valid_indices] = scattered_items\n",
        "\n",
        "    return tf.constant(scattered_seq, dtype=seq.dtype)\n",
        "\n",
        "\n",
        "def test_simple_version():\n",
        "    \"\"\"测试简化版本\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"测试简化版类目打散函数\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    seq = tf.constant([\n",
        "        [101, 102, 201, 202, 301, 302, 103, 203],\n",
        "        [401, 402, 501, 502, 601, 602, 403, 503]\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    category_seq = tf.constant([\n",
        "        [1, 1, 2, 2, 3, 3, 1, 2],\n",
        "        [4, 4, 5, 5, 6, 6, 4, 5]\n",
        "    ], dtype=tf.int32)\n",
        "\n",
        "    print(\"原始序列:\")\n",
        "    print(seq.numpy())\n",
        "    print(\"类目序列:\")\n",
        "    print(category_seq.numpy())\n",
        "\n",
        "    result = apply_category_scatter_simple(seq, category_seq)\n",
        "    print(\"打散后序列:\")\n",
        "    print(result.numpy())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 设置TensorFlow日志级别\n",
        "    tf.get_logger().setLevel('INFO')\n",
        "\n",
        "    # 运行测试\n",
        "    test_simple_version()\n",
        "\n",
        "    # 注意：完整的TensorFlow版本可能需要进一步调试\n",
        "    # 因为在graph模式下，Python的for循环和列表操作可能不适用\n",
        "    print(\"\\n提示：完整的TensorFlow版本需要使用tf.while_loop等操作\")\n",
        "    print(\"建议先使用简化版本理解算法逻辑，再转换为TensorFlow操作\")\n",
        ""
      ],
      "metadata": {
        "id": "X4dY6P42zh3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9l9Yku4zd0F"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch as th\n",
        "\n",
        "from glide_text2im.download import load_checkpoint\n",
        "from glide_text2im.model_creation import (\n",
        "    create_model_and_diffusion,\n",
        "    model_and_diffusion_defaults,\n",
        "    model_and_diffusion_defaults_upsampler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmf5Lf4gzd0F"
      },
      "outputs": [],
      "source": [
        "# This notebook supports both CPU and GPU.\n",
        "# On CPU, generating one sample may take on the order of 20 minutes.\n",
        "# On a GPU, it should be under a minute.\n",
        "\n",
        "has_cuda = th.cuda.is_available()\n",
        "device = th.device('cpu' if not has_cuda else 'cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEwV8oSAzd0G"
      },
      "outputs": [],
      "source": [
        "# Create base model.\n",
        "options = model_and_diffusion_defaults()\n",
        "options['use_fp16'] = has_cuda\n",
        "options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n",
        "model, diffusion = create_model_and_diffusion(**options)\n",
        "model.eval()\n",
        "if has_cuda:\n",
        "    model.convert_to_fp16()\n",
        "model.to(device)\n",
        "model.load_state_dict(load_checkpoint('base', device))\n",
        "print('total base parameters', sum(x.numel() for x in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FtmVPXAzd0G"
      },
      "outputs": [],
      "source": [
        "# Create upsampler model.\n",
        "options_up = model_and_diffusion_defaults_upsampler()\n",
        "options_up['use_fp16'] = has_cuda\n",
        "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
        "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
        "model_up.eval()\n",
        "if has_cuda:\n",
        "    model_up.convert_to_fp16()\n",
        "model_up.to(device)\n",
        "model_up.load_state_dict(load_checkpoint('upsample', device))\n",
        "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZSCwXpvzd0G"
      },
      "outputs": [],
      "source": [
        "def show_images(batch: th.Tensor):\n",
        "    \"\"\" Display a batch of images inline. \"\"\"\n",
        "    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
        "    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
        "    display(Image.fromarray(reshaped.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gvA5Ksyzd0G"
      },
      "outputs": [],
      "source": [
        "# Sampling parameters\n",
        "prompt = \"an oil painting of a corgi\"\n",
        "batch_size = 1\n",
        "guidance_scale = 3.0\n",
        "\n",
        "# Tune this parameter to control the sharpness of 256x256 images.\n",
        "# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
        "upsample_temp = 0.997"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H808n00Azd0G"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Sample from the base model #\n",
        "##############################\n",
        "\n",
        "# Create the text tokens to feed to the model.\n",
        "tokens = model.tokenizer.encode(prompt)\n",
        "tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the classifier-free guidance tokens (empty)\n",
        "full_batch_size = batch_size * 2\n",
        "uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
        "    [], options['text_ctx']\n",
        ")\n",
        "\n",
        "# Pack the tokens together into model kwargs.\n",
        "model_kwargs = dict(\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size + [uncond_mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Create a classifier-free guidance sampling function\n",
        "def model_fn(x_t, ts, **kwargs):\n",
        "    half = x_t[: len(x_t) // 2]\n",
        "    combined = th.cat([half, half], dim=0)\n",
        "    model_out = model(combined, ts, **kwargs)\n",
        "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
        "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
        "    eps = th.cat([half_eps, half_eps], dim=0)\n",
        "    return th.cat([eps, rest], dim=1)\n",
        "\n",
        "# Sample from the base model.\n",
        "model.del_cache()\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model_fn,\n",
        "    (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        ")[:batch_size]\n",
        "model.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VQwdOPOzd0H"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Upsample the 64x64 samples #\n",
        "##############################\n",
        "\n",
        "tokens = model_up.tokenizer.encode(prompt)\n",
        "tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
        "    tokens, options_up['text_ctx']\n",
        ")\n",
        "\n",
        "# Create the model conditioning dict.\n",
        "model_kwargs = dict(\n",
        "    # Low-res image to upsample.\n",
        "    low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
        "\n",
        "    # Text tokens\n",
        "    tokens=th.tensor(\n",
        "        [tokens] * batch_size, device=device\n",
        "    ),\n",
        "    mask=th.tensor(\n",
        "        [mask] * batch_size,\n",
        "        dtype=th.bool,\n",
        "        device=device,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Sample from the base model.\n",
        "model_up.del_cache()\n",
        "up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
        "up_samples = diffusion_up.ddim_sample_loop(\n",
        "    model_up,\n",
        "    up_shape,\n",
        "    noise=th.randn(up_shape, device=device) * upsample_temp,\n",
        "    device=device,\n",
        "    clip_denoised=True,\n",
        "    progress=True,\n",
        "    model_kwargs=model_kwargs,\n",
        "    cond_fn=None,\n",
        ")[:batch_size]\n",
        "model_up.del_cache()\n",
        "\n",
        "# Show the output\n",
        "show_images(up_samples)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "e7d6e62d90e7e85f9a0faa7f0b1d576302d7ae6108e9fe361594f8e1c8b05781"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}